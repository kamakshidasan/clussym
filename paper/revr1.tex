\documentclass[10pt]{article}
\usepackage{color}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}
\setlength{\headheight}{0.05in}
\setlength{\headsep}{0.05in}
\setlength\paperheight {10.75in}
\setlength\paperwidth  {7.875in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\title{}
\date{}

\begin{document}
\noindent Dear IEEE Visulization Reviewers,\\\\

We thank all reviewers for their detailed comments. We have revised the paper to address the issues raised by the reviewers.
Below, we respond to all reviewer comments. The main changes are highlighted in the paper also.\\\\

{\noindent \LARGE Response to Summary Review}\\

The paper is based on many steps that are only provided in form of
   references, e.g., the computation of a contour tree. However, this cannot
   be avoided. The only thing that I would have liked to see added is an
   equation for the descriptor based on the Laplace-Beltrami spectra.

   {\color{blue}Added this in Section 5.2}\\

   One comment:
   Since the performance depends on the number of critical points and the
   number of contours, they should be listed in Table 1.

   The paper is technically sound and discusses almost all relevant aspects.
   I only have a few comments (many are ``nice to have'' but not crucial):\\

   Assuming that we have symmetric regions that are not perfectly matching
   in terms of the isosurfaces like in Fig. 10b. What if there is no single
   isovalue that captures all symmetries before the components falling
   apart? Can we still find the symmetries? One step further: What if we
   have to use different isovalues for the different arcs to have matching
   symmetries? Can that be captured?\\
   
   The authors describe their work as a general framework and mention that
   different shape descriptors can be used. Other examples than the
   implemented one are mentioned. It would have been nice to see the use of
   others, as well. 

   {\color{blue} We were unable to find software libraries that have implemented
	robust shape descriptors. It is beyond the scope of this review cycle
	to implement shape descriptors from scratch. Hence, we have not
performed experiments with other shape descriptors.}\\

   It is not clear from Table 1, how the algorithm scales with the number
   of critical points or contours. Respective plots would be helpful.

   The Laplace-Beltrami spectra are computed after simplifying the meshes.
   How (if at all) does this affect the result? How much can the meshes be
   simplified?

   {\color{blue}As described in Section 5.2, we set a budget of 1000 vertices
	   for mesh simplification. For the datasets which we have considered,
	   simplification does not affect the results. In general, it is difficult
	   to estimate upfront how much the meshes can be simplified. A possibility
	   is to make use of the error metric used by the simplification algorithm
	   and stop simplification when the error exceeds a threshold. However, we have
	   not explored this approach since evaluating different strategies for simplification
	   and its effect on shape descriptors is a subject in itself and is beyond the scope
   of this work.}\\

   Robustness against noise has been given as a main advantage of this
   approach. It would have been nice to see how much noise can be handled by
   taking a noise-free (synthetic) data set and successively add noise until
   it fails. Also, the change of the shape descriptor space would have been
   interesting to see.



{\noindent \LARGE Response to Reviewer 2}\\\\

	In the following I summarize some questions and comments that could be
   further discussed in the paper:

   The method requires a proper treatment of noise, which is done by
   counting vertices of contours. This requires a continuously sampled
   scalar field, this could be mentioned.

   {\color{blue}Added this in Section 4.2}\\

   - Shape comparison using the Laplace-Beltrami spectrum:

   How many modes to you use for the comparison. This number determines
   the dimension of the descriptor space and thus the performance of the
   clustering step. For high dimensions it may be possible that the
   distribution of points in the descriptor space is very spars and a simple
   neighborhood search might not be sufficient. 

   {\color{blue}We state in Section 5.2 that we use the first ten non-zero eigen
	   values as the shape descriptor. We agree that the low dimension
	   may be the reason why
	   nearest neighbour search works well for us 
	   and for higher dimensions a more sophisticated clustering
   scheme and distance metric may be needed.}\\

   Is the Euclidean distance a meaningful distance measure in this space?
   The order of magnitude of the single eigenvalues might differ
   significantly. You might consider a lexicographic distance measure
   instead.

   {\color{blue}We mention in Section 5.3 that we use Euclidean distance
	   since it has been used in the past and has yielded good results.
	   However, this is not the ideal metric since the first few eigen
	   values are more important than the rest. This does not affect our
	   algorithm in a big way since we use only the first ten non-zero
	   eigen values. As described in Section 5.3, a better metric may
   be determined using methods like metric learning.}\\

   The statement that two different shapes have a different
   Laplace-Beltrami spectrum is only partially true. The spectrum is
   invariant with respect to isometric transformations.  

   {\color{blue} We agree. In geometry processing community,
	   isometric shapes are also considered to be symmetric (called instrinsic symmetry).
   Therefore, we feel it is not wrong to classify isometric contours as symmetric.}\\

   The cotangent weighted scheme for the computation of the
   Laplace-Beltrami spectrum is very sensitive with respect to the quality
   of the triangulation mesh. Could you please discuss related issues. Many
   simple isosurface computation algorithms have some badly shaped
   triangles.

   {\color{blue}In Section 5.2, we state that in case of bad triangulations,
	   mesh quality aware isosurface generation or remeshing may have to
   be performed.}\\


   It is stated that it is observed that the clusters in the descriptor
   space are well separated. This might be due to the choice of data sets
   that you considered for your evaluation. All these datasets exhibit a
   nice inherent symmetry. Looking at a more general data set this might not
   be the case.

   {\color{blue} We agree. For more complex datasets, a more sophisticated
   shape descriptor may be needed.}\\

	
   Most examples used for the evaluation show impressive results. This are
   data sets which show strong symmetries, that are well known in advance.
   In contrast the Isabel data set is not really convincing and seems a
   little bit artificial. Could you discuss in more details for what
   scenarios this is a meaningful approach and where are the limitations.

   {\color{blue}The intention of using Isabel dataset was to show that the descriptor space representation
	   can be used for other tasks like query driven exploration. We highlight in Section 8
	   that applications like searching and tracking will benefit from the descriptor
   space representation and plan to explore this in our future work.}\\

{\noindent \LARGE Response to Reviewer 1}\\\\
   First, I would not say that the paper addresses symmetry detection. In my
   opinion, it addresses partial self-similarity detection. At no point in
   the paper the algorithm studies or makes use of the transformation that
   maps a region to another. The technique seems oblivious to the type of
   symmetry (reflective or rotational symmetry). For instance, if a scalar
   field is given such that a contour is repeated in random locations of
   space, I suspect the proposed technique will identify the original
   contour and its repetitions as being symmetric although their layout in
   space is completely random. Thus I would encourage the authors to modify
   the title to better reflect this. The application described in section
   6.1 also goes in the direction of this remark.

	
   {\color{blue}We explain in Section 1 that for scalar fields it is more
	   meaningful to use a broader definition of symmetry that captures
	   all repeating occurrences of a pattern. This is also highlighted in
	   Section 3. We prefer to not use the term similarity detection
	   since similarity detection is usually associated with pairs of regions
   	   while our method identifies a collection of regions which are similar.}\\

   Still, the authors show a few visual comparisons with the approach by
   Thomas and Natarajan and show that their technique is additionally able
   to detect "nested" symmetries (more than multi-scale).
   I found the application described in Figure 8 particularly relevant and I
   believe it is probably the most interesting result of the paper. However,
   it is unclear if such an application couldn't be derived with previous
   techniques such as the one by Thomas and Natarajan.

   {\color{blue}An advantage of using our method is that the distance between
	   points in the descriptor space gives an indication of the asymmetry
	   present within symmetric contours. However, we do not claim that
	   asymmetry detection is unique to our method. It is possible that
   earlier methods can also perform the same task.}\\

The paper is very well written and the exposition is mostly clear except
   for the discussion of the clustering algorithm. In section 5.3, the
   authors present two strategies (nearest neighbors and spectral
   clustering) and it's not clear which one is used in the remainder of the
   paper.

   {\color{blue}All the results were generated using nearest neighbor search.
	   To avoid confusion, we have dropped the discussion on spectral
	   clustering in Section 5 (implementation) and retain the discussion
	   on Section 4 which points readers interested in the implementation
   to [15].}\\
   
   How did the authors simplify the contour surfaces prior to computing
   their spectra? The simplification strategies here can have a major impact
   on the subsequent similarity estimation.

   {\color{blue}This is added to Section 5.2}

   When reading the paper, I was concerned with computation time
   performance, especially for the clustering side of things. I would
   encourage the authors to detail this part in table 1 or in a new table.
   For instance, maximal clique identification in a graph is computationally
   expensive. Also, at the end of section 4.4, the authors evaluate the
   maximum weight matching in the bipartite graph of contours' children.
   This part is also computationally expensive. I was really surprised to
   read in the text that the clustering part takes less than a 
   second. I wish the authors could detail for each data-set the number of 
   contours extracted from the contour tree, which directly impacts the
   run-time of the maximum clique computation and maximum weight matching.
   How do the clustering performances scale when this number increases for
   large data-sets or when the simplification level of the contour tree is
   kept minimal (to maintain small-scale features in the analysis)?
   Also, I believe comparing Laplace-Beltrami spectra only makes sense from
   a geometrical point of view if the two surfaces have the same topology.
   Such a thing cannot be guaranteed with scalar field contours due to
   boundary components or different genus. Does it affect the similarity
   evaluation performances in practice?

   I believe it would have been nice to further experiment the robustness of
   the techniques with synthetic data-sets, designed to assess specific
   features of the algorithm (for instance, robustness to noise, ambiguous
   symmetries, etc.)

   Also, the title (and other remarks throughout the paper) suggests that
   the approach can detect symmetries at multiple scale. I didn't find any
   evidence of this in the experiments. Can the approach detect symmetries
   between regions of similar shapes but drastically different volumes (i.e.
   scale)?
   It seems that the authors make a mis-usage of the word multi-scale since
   they seem to refer to the ability of their technique to detect
   similarities between groups of nested contours. This is misleading.

   {\color{blue}The usage of the term ``multiscale symmetry detection'' in
	   the geometry processing community refers to symmetry detection of
	   different sets of regions of different sizes where the size of the regions
	   within each set is the same. (e.g. Multi-Scale Partial Intrinsic Symmetry Detection,
   ACM Transactions on Graphics (SIGGRAPH Asia 2012) by Kai Xu et al.
   We follow the same convention in the usage of the term. ``multiscale''.
   We have also rephrased the sentences that introduce the term ``multiscale''
   in Section 1 to make this clear. On a related note, it is possible to
   detect similar shapes with different volumes using a scale invariant descriptor  as we describe in Section 6.1}\\
   

{\noindent \LARGE Response to Reviewer 4}\\\\
   The authors should provide a better justification of their definition of
   symmetry. It is not clear how the definition based on shape descriptors
   that they introduce is related to the one based on symmetry
   transformation. \\

   {\color{blue}This is now clarified in Section 1 and Section 3.}\\

   Also, the authors say that the approach proposed in the paper is related
   to the work of Bruckner et al.. They provide a general comparison in the
   related work. I think that a more in-depth comparison should be performed
   at the end of the paper after that the proposed method and the results
   have been described.
   The authors provide a comparison with previous approaches in the related
   work. I think that there is too much detail here since at this point we
   do not know the details of the proposed approach. My suggestion is to
   postpone some of the comments and have a more in-depth comparison in
   particular with the method by Bruckner et al.

   {\color{blue}We clarify in Section 2 that although the method by Bruckner et al.
	   is related to our method, the goals of the two methods are quite different.
	   Hence, it is not meaningful to do a more in-depth comparison of the results
	   of the two methods at the end of the paper. Therefore, we only provide
	   a general comparion in Section 2. Moving it to a later section will
   disrupt the organization of the contents.}

\end{document}
